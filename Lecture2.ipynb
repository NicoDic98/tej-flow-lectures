{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01ab6a8-6340-4941-b73c-777380ab04c2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0967740-b23a-4bde-8250-b281e1ff41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45280a5-9a53-40c7-8f0b-08adbd271907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(x):\n",
    "    \"\"\"Convert torch tensor to numpy array\"\"\"\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57459579-270f-4b9c-bb52-8c7b2111dcf2",
   "metadata": {},
   "source": [
    "# Brief ML primer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9dfa1-8905-45c0-9c0f-e84b95aee98a",
   "metadata": {},
   "source": [
    "Let's quickly demonstrate how training looks in Pytorch. We will train a small neural network to model the function\n",
    "$$\n",
    "f(x) = \\mathrm{sinc}(x) := \\frac{\\sin(\\pi x)}{\\pi x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c40669-534d-4035-821d-7358286d615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_fn(x):\n",
    "    return torch.sinc(x)\n",
    "fig, ax = plt.subplots(1,1, figsize=(3.5, 2.5))\n",
    "xs = torch.linspace(-5, 5, steps=51)\n",
    "ys = target_fn(xs)\n",
    "ax.plot(grab(xs), grab(ys))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01596f55-1486-4dc6-a583-ab81c885f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 1, 'x should just have a batch index'\n",
    "        return self.net(x[:,None])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81b3d4-32db-4731-9f56-f5bced8d7808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = ToyModel()\n",
    "    batch_size = 128\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_hist = []\n",
    "    for i in tqdm.tqdm(range(25000)):\n",
    "        opt.zero_grad()\n",
    "        # random samples around 0 for the training points\n",
    "        x = 3*torch.randn((batch_size,))\n",
    "        model_y = model(x)\n",
    "        true_y = target_fn(x)\n",
    "        # mean squared error\n",
    "        loss = ((true_y - model_y)**2).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_hist.append(grab(loss))\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f'Step {i+1}: Loss {grab(loss)}')\n",
    "    return dict(model=model, loss=np.stack(loss_hist))\n",
    "res = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05164c5c-0de1-4ab1-a440-2f07b8a27a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(6.5, 2.5))\n",
    "xs = torch.linspace(-5, 5, steps=51)\n",
    "true_ys = target_fn(xs)\n",
    "model_ys = res['model'](xs)\n",
    "ax = axes[0]\n",
    "ax.plot(grab(xs), grab(true_ys), color='k')\n",
    "ax.plot(grab(xs), grab(model_ys), color='xkcd:red')\n",
    "ax = axes[1]\n",
    "ax.plot(res['loss'])\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0306f-b6fd-4a46-87ae-2b6243e5d046",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d6a36-73d2-48d1-8a82-d431cb71318d",
   "metadata": {},
   "source": [
    "We will use a standard 2D target, a mixture of Gaussians:\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2\\pi}} \\sum_i \\sigma(\\alpha)_i e^{-(x-\\mu_i)^2/2}.\n",
    "$$\n",
    "Above, $\\sigma(\\alpha)_i := e^{\\alpha_i} / \\sum_j e^{\\alpha_j}$ is the Softmax function, which maps a vector $\\vec{\\alpha}$ to positive-definite and normalized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c5295-4f73-47a1-8c80-8de19bd4f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfGaussians:\n",
    "    def __init__(self, mu, alpha):\n",
    "        assert mu.shape[-1] == 2\n",
    "        assert mu.shape[0] == len(alpha)\n",
    "        self.mu = mu\n",
    "        self.logw = torch.nn.functional.log_softmax(alpha, dim=0)\n",
    "    def log_prob(self, x):\n",
    "        terms = self.logw -((x-self.mu)**2/2).sum(-1)\n",
    "        return -0.5*np.log(2*np.pi) + torch.logsumexp(terms, dim=0)\n",
    "    def sample(self, batch_size):\n",
    "        x = torch.randn((batch_size,2))\n",
    "        i = np.arange(len(self.logw))\n",
    "        inds = np.random.choice(i, size=batch_size, p=np.exp(grab(self.logw)))\n",
    "        # this indexing trick extracts the full 2d vectors at inds (see below)\n",
    "        return x + self.mu[inds[:,None], np.arange(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc883f-d65c-4f87-be0b-e1012d70f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an indexing trick that will be useful\n",
    "a = np.arange(24).reshape(6, 4)\n",
    "inds = np.array([1, 4, 3])\n",
    "# this pulls rows \"inds\" out of a\n",
    "extracted = a[inds[:,None], np.arange(4)]\n",
    "print(f'{a=}')\n",
    "print(' -> ')\n",
    "print(f'{extracted=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f5083-3238-47fb-a881-27946b42758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor([\n",
    "    [5., 0.],\n",
    "    [0, 5.],\n",
    "    [-5., -3.]\n",
    "])\n",
    "alpha = torch.tensor([0.5, 0.0, 0.8])\n",
    "target = MixtureOfGaussians(mu, alpha)\n",
    "# samples\n",
    "xs = grab(target.sample(4096))\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "bins = np.linspace(-8, 8, num=51)\n",
    "axes[0].hist2d(xs[:,0], xs[:,1], bins=bins)\n",
    "# density\n",
    "x_mesh = torch.stack(torch.meshgrid(torch.tensor(bins), torch.tensor(bins)), dim=-1)\n",
    "x_flat = x_mesh.reshape(-1, 2)\n",
    "logq = grab(torch.func.vmap(target.log_prob)(x_flat).reshape(51, 51))\n",
    "x_mesh = grab(x_mesh)\n",
    "axes[1].contourf(x_mesh[...,0], x_mesh[...,1], np.exp(logq))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect(1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91778d6c-d527-4672-ba26-f48af440297d",
   "metadata": {},
   "source": [
    "# Flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624246f-6b86-456a-8002-5aaedc7dc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Velocity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 2),\n",
    "        )\n",
    "    def value(self, x, t):\n",
    "        # net expects a batch index\n",
    "        inp = torch.cat([x, t[None]])\n",
    "        return self.net(inp[None])[0]\n",
    "    def div(self, x, t):\n",
    "        # NOTE: for high dimensions, this is expensive!\n",
    "        J = torch.func.jacfwd(self.value, argnums=0)(x, t)\n",
    "        # div = trace of Jacobian\n",
    "        return torch.trace(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c669f-1a79-4297-a840-5958c05f5725",
   "metadata": {},
   "source": [
    "From Lecture 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4ed3b-9f31-4e7c-98f5-b4b30d7f019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow(x, velocity, *, n_step, tf=1.0, inverse=False):\n",
    "    dt = tf/n_step\n",
    "    ts = dt*torch.arange(n_step)\n",
    "    logJ = torch.tensor(0.0)\n",
    "    sign = 1\n",
    "    if inverse:\n",
    "        sign = -1\n",
    "        ts = reversed(ts)\n",
    "    for t in ts:\n",
    "        # transport samples\n",
    "        x = x + sign * dt * velocity.value(x, t)\n",
    "        # estimate change of measure\n",
    "        logJ = logJ + dt * velocity.div(x, t)\n",
    "    return x, logJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671b04a-0362-4195-847d-72857c9ab89b",
   "metadata": {},
   "source": [
    "**Training:** Reverse KL divergence between target and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7be17-56be-4050-b3ba-e033e634338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_rkl():\n",
    "    torch.manual_seed(1234)\n",
    "    model = Velocity()\n",
    "    batch_size = 32\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_hist = []\n",
    "    flow_batch = torch.func.vmap(flow, in_dims=(0, None))\n",
    "    target_batch = torch.func.vmap(target.log_prob)\n",
    "    for i in tqdm.tqdm(range(1000)):\n",
    "        opt.zero_grad()\n",
    "        # sample prior\n",
    "        xr = torch.randn((batch_size, 2))\n",
    "        logr = -(xr**2/2).sum(-1) - np.log(2*np.pi)\n",
    "        # flow\n",
    "        x, logJ = flow_batch(xr, model, n_step=10)\n",
    "        logq = logr - logJ\n",
    "        # kl divergence\n",
    "        logp = target_batch(x)\n",
    "        loss = (logq - logp).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_hist.append(grab(loss))\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f'Step {i+1}: Loss {grab(loss)}')\n",
    "    return dict(model=model, loss=np.stack(loss_hist))\n",
    "res_rkl = train_model_rkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0aff2-edb8-4ec2-8c34-bdea7add13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(3.5, 2.5))\n",
    "ax.plot(res_rkl['loss'])\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('iter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e01bdf-b969-416a-a79e-a6974425f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_batch = torch.func.vmap(flow, in_dims=(0, None))\n",
    "xr = torch.randn((4096, 2))\n",
    "x, logJ = flow_batch(xr, res_rkl['model'], n_step=10)\n",
    "bins = np.linspace(-8, 8, num=51)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6.5, 2.5))\n",
    "axes[0].hist2d(grab(x)[:,0], grab(x)[:,1], bins=bins)\n",
    "axes[0].set_title('model samples')\n",
    "x2 = grab(target.sample(4096))\n",
    "axes[1].hist2d(x2[:,0], x2[:,1], bins=bins)\n",
    "axes[1].set_title('true samples')\n",
    "for ax in axes:\n",
    "    ax.set_aspect(1.0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
