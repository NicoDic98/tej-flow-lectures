{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411d6427-4823-404d-b561-64cb2cf84628",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b86e79-a80a-4e1e-9524-3ab4209fa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692291bf-7d91-40ec-a641-7a853fcc8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290692d-4e46-4163-8e33-f73fbe259397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ess(w):\n",
    "    return (w.mean()**2) / (w**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e2e0e-ebd1-42e4-9b13-d9f5a99c5367",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Phi4 theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a287c-45ea-4cce-b150-8af644e48425",
   "metadata": {},
   "source": [
    "Copied from Lecture 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d93fb0-78d5-4c37-9386-594e049509ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi4Action:\n",
    "    def __init__(self, m2, lam):\n",
    "        self.m2 = m2\n",
    "        self.lam = lam\n",
    "        self.grad = torch.func.grad(self.value)\n",
    "    def value(self, phi):\n",
    "        # phi.shape = (Lx, Ly, ...)\n",
    "        Nd = len(phi.shape)\n",
    "        S = ((Nd + self.m2/2) * phi**2 + (self.lam/24) * phi**4).sum()\n",
    "        for mu in range(Nd):\n",
    "            phi_fwd = torch.roll(phi, -1, dims=mu)\n",
    "            S -= (phi * phi_fwd).sum()\n",
    "        return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb8c9d-2aa0-45fd-b403-9d660c716381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog_update(phi, pi, action, *, dt, n_leap):\n",
    "    phi += (dt/2)*pi\n",
    "    for _ in range(n_leap-1):\n",
    "        pi -= dt*action.grad(phi)\n",
    "        phi += dt*pi\n",
    "    pi -= dt*action.grad(phi)\n",
    "    phi += (dt/2)*pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7917474-109d-464e-a899-b6a136e558c1",
   "metadata": {},
   "source": [
    "Modified to keep L as a parameter, and return samples $\\phi$ and associated action values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7e79d-0aa6-4567-afb1-a22d30227e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hmc(action, *, L, n_therm, n_iter, n_meas, dt=0.10, n_leap=10):\n",
    "    torch.manual_seed(1234)\n",
    "    phi = 0.1*torch.randn((L, L)) # warm start\n",
    "    S = action.value(phi)\n",
    "    acc = 0\n",
    "    tot = 0\n",
    "    meas = []\n",
    "    phis = []\n",
    "    actions = []\n",
    "    for i in tqdm.tqdm(range(-n_therm, n_iter)):\n",
    "        new_phi = phi.clone()\n",
    "        pi = torch.randn_like(phi)\n",
    "        K = (pi**2/2).sum()\n",
    "        leapfrog_update(new_phi, pi, action, dt=dt, n_leap=n_leap)\n",
    "        Sp = action.value(new_phi)\n",
    "        Kp = (pi**2/2).sum()\n",
    "        dH = grab(Sp + Kp - S - K)\n",
    "        tot += 1\n",
    "        if np.random.random() < np.exp(-dH): # accept\n",
    "            phi = new_phi\n",
    "            S = Sp\n",
    "            acc += 1\n",
    "        if i >= 0 and (i+1)%n_meas == 0:\n",
    "            meas.append(grab(phi.mean()))\n",
    "            phis.append(grab(phi))\n",
    "            actions.append(S)\n",
    "            # print(f'Acc {100.0*acc/tot:.2f}')\n",
    "    return dict(meas=np.stack(meas), phis=np.stack(phis), actions=np.stack(actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d3cbc-1f05-4823-a3c1-963a0e6c33ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab071ad-eeac-43bf-ba3c-2c4788ed6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Velocity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_kwargs = dict(kernel_size=3, padding=1, padding_mode='circular')\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(2, 8, **conv_kwargs),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(8, 8, **conv_kwargs),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(8, 1, **conv_kwargs),\n",
    "        )\n",
    "    def value(self, x, t):\n",
    "        inp = torch.stack([x, torch.ones_like(x)*t])\n",
    "        return self.net(inp)[0]\n",
    "    def div(self, x, t):\n",
    "        shape = x.shape\n",
    "        x_flat = x.flatten()\n",
    "        def eval_flat(y):\n",
    "            x = y.reshape(shape)\n",
    "            inp = torch.stack([x, torch.ones_like(x)*t])\n",
    "            return self.net(inp)[0].flatten()\n",
    "        J = torch.func.jacfwd(eval_flat)(x_flat)\n",
    "        return torch.trace(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d42a48-88a2-4a0b-a620-b752e2739b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity = Velocity()\n",
    "phi = torch.randn((4, 4))\n",
    "print(velocity.value(phi, 1.0))\n",
    "print(velocity.div(phi, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31089e06-8da5-49eb-b0d8-ad99dcc8e929",
   "metadata": {},
   "source": [
    "Copied from Lecture 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981b9d7-73bf-4077-b297-7f9551cb1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow(x, velocity, *, n_step, tf=1.0, inverse=False):\n",
    "    dt = tf/n_step\n",
    "    ts = dt*torch.arange(n_step)\n",
    "    logJ = torch.tensor(0.0)\n",
    "    sign = 1\n",
    "    if inverse:\n",
    "        sign = -1\n",
    "        ts = reversed(ts)\n",
    "    for t in ts:\n",
    "        # transport samples\n",
    "        x = x + sign * dt * velocity.value(x, t)\n",
    "        # estimate change of measure\n",
    "        logJ = logJ + dt * velocity.div(x, t)\n",
    "    return x, logJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163261e0-4806-4383-bae5-50ad9e34da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk size makes sure we do not try to evaluate all samples at\n",
    "# the same time, exhausting our memory\n",
    "flow_batch = torch.func.vmap(flow, in_dims=(0, None), chunk_size=512)\n",
    "flow_batch(torch.randn((1, 4, 4)), velocity, n_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dd257-dc95-4db5-8375-65bfaf1562e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our target will be in the broken phase\n",
    "target = Phi4Action(-0.5, 1.5)\n",
    "res_hmc = run_hmc(target, L=4, n_therm=100, n_iter=1000, n_meas=1, dt=0.05, n_leap=20)\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,2.5))\n",
    "ax.plot(res_hmc['meas'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214aabf4-5d68-496a-89ae-a752c0ed4131",
   "metadata": {},
   "source": [
    "We will approach the target by learning 5 different flows trained to transport between the $m^2 = 0, \\lambda = 1.5$ theory (symmetric phase) and the $m^2 = -0.5, \\lambda = 1.5$ theory (broken phase) in steps of $m^2$:\n",
    "$$\n",
    "(m^2, \\lambda) = (0, 1.5) \\; \\longrightarrow \\; (-0.1, 1.5) \\; \\longrightarrow \\; (-0.2, 1.5) \\; \\longrightarrow \\; (-0.3, 1.5) \\; \\longrightarrow \\; (-0.4, 1.5) \\; \\longrightarrow \\; (-0.5, 1.5)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e01af-d2da-4bda-a51c-9a09e0bc4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [Phi4Action(m2, 1.5) for m2 in [0.0, -0.1, -0.2, -0.3, -0.4, -0.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9986f-6b0f-43c4-be1b-a462255af02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(prior, target, *, batch_size=4, n_iter=1000):\n",
    "    torch.manual_seed(1234)\n",
    "    model = Velocity()\n",
    "    L = 4\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    loss_hist = []\n",
    "    ess_hist = []\n",
    "    flow_batch = torch.func.vmap(flow, in_dims=(0, None))\n",
    "    target_batch = torch.func.vmap(target.value)\n",
    "    prior_batch = torch.func.vmap(prior.value)\n",
    "    res_hmc = run_hmc(prior, L=L, n_therm=100, n_iter=1000, n_meas=1, dt=0.05, n_leap=20)\n",
    "    prior_samples = res_hmc['phis']\n",
    "    prior_actions = res_hmc['actions']\n",
    "    for i in tqdm.tqdm(range(n_iter)):\n",
    "        opt.zero_grad()\n",
    "        # sample prior\n",
    "        inds = np.random.randint(len(prior_samples), size=batch_size)\n",
    "        xr = torch.tensor(prior_samples[inds])\n",
    "        logr = -torch.tensor(prior_actions[inds])\n",
    "        # flow\n",
    "        x, logJ = flow_batch(xr, model, n_step=10)\n",
    "        logp = -target_batch(x)\n",
    "        # ordinary kl logq\n",
    "        # logq = logr - logJ\n",
    "        # path grad logq\n",
    "        model.requires_grad_(False)\n",
    "        xr2, logJ2 = flow_batch(x, model, n_step=10, inverse=True)\n",
    "        model.requires_grad_(True)\n",
    "        logq = -prior_batch(xr2) - logJ2\n",
    "        # kl div\n",
    "        loss = (logq - logp).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_hist.append(grab(loss))\n",
    "        ess = compute_ess((logp - logq).exp())\n",
    "        ess_hist.append(grab(ess))\n",
    "        if (i+1) % 25 == 0:\n",
    "            print(f'Step {i+1}: Loss {grab(loss)} ESS {grab(ess)}')\n",
    "    fig, axes = plt.subplots(1,2, figsize=(8, 3))\n",
    "    axes[0].plot(loss_hist)\n",
    "    axes[1].plot(ess_hist)\n",
    "    axes[0].set_ylabel('loss')\n",
    "    axes[1].set_ylabel('ess')\n",
    "    return dict(model=model, loss=np.stack(loss_hist), ess=np.stack(ess_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea09f7-367c-42e6-8cc7-b1cf9443954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    train_model(targets[i], targets[i+1], batch_size=32, n_iter=10)\n",
    "    for i in range(len(targets)-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4bdac-c37e-4760-b166-e92060aa04d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41453f-26ed-4d26-a7e9-d3c91eaf4491",
   "metadata": {},
   "source": [
    "## L = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ef365-7907-4d00-90ee-46a3b42a73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Phi4Action(0.0, 1.5)\n",
    "phi_r = torch.tensor(run_hmc(prior, L=4, n_therm=100, n_iter=5000, n_meas=2, dt=0.05, n_leap=20)['phis'])\n",
    "logr = -torch.func.vmap(prior.value)(phi_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25d511-2e62-4bdb-82b1-f9bd3b4da451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_flows(phi_r):\n",
    "    samples = [phi_r]\n",
    "    ws = [torch.ones(phi_r.shape[0])]\n",
    "    for res, target_p, target in zip(tqdm.tqdm(results), targets[:-1], targets[1:]):\n",
    "        with torch.no_grad(): # we don't need gradients, so don't waste memory for it\n",
    "            phi, logJ = flow_batch(samples[-1].clone(), res['model'], n_step=100)\n",
    "        dlogq = -logJ\n",
    "        dlogp = -torch.func.vmap(target.value)(phi) + torch.func.vmap(target_p.value)(samples[-1])\n",
    "        ws.append(ws[-1] * (dlogp - dlogq).exp())\n",
    "        samples.append(phi)\n",
    "        print(f'ess: {compute_ess(ws[-1])}')\n",
    "    return dict(samples=samples, ws=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdec979-307c-48e1-b341-aca5cd1c7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_L4 = apply_flows(phi_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86caae2a-0ae2-466e-b835-25aac6022ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with 2500 samples from the HMC\n",
    "target = Phi4Action(-0.5, 1.5)\n",
    "res_hmc_L4 = []\n",
    "fig, ax = plt.subplots(1,1, figsize=(6, 3))\n",
    "for m2 in [0.0, -0.1, -0.2, -0.3, -0.4, -0.5]:\n",
    "    res_hmc_L4.append(run_hmc(Phi4Action(m2, 1.5), L=4, n_therm=100, n_iter=5000, n_meas=2, dt=0.05, n_leap=20))\n",
    "    ax.plot(res_hmc_L4[-1]['meas'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986a759-bd83-412f-9d80-8272b6399b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3)\n",
    "axes = axes.flatten()\n",
    "bins = np.linspace(-2, 2, 21)\n",
    "for ax, phi, w, hmc in zip(axes, res_L4['samples'], res_L4['ws'], res_hmc_L4):\n",
    "    ax.hist(grab(phi.flatten(1).mean(-1)), color='k', density=True, histtype='step', bins=bins, linestyle='--')\n",
    "    ax.hist(grab(phi.flatten(1).mean(-1)), weights=grab(w), density=True, bins=bins)\n",
    "    ax.hist(hmc['meas'], density=True, bins=bins, alpha=0.5, color='xkcd:red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b275fac-ca23-4c19-895f-0418e4b0b8b1",
   "metadata": {},
   "source": [
    "## L = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f94fb9-f1d2-4678-a407-3f3a38e823e7",
   "metadata": {},
   "source": [
    "Because we worked with convolutions, the _same_ flow model can be directly applied to the L=8 theory. This is one of the exercises!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
